{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester_scale import test_classifier, dump_classifier_and_data\n",
    "\n",
    "#additional imports\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import enron_tools\n",
    "import enron_evaluate\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "import enron_evaluate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sep = '##############################################################################################'\n",
    "sep2 = '++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++'\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### create list of functions for use as argument to add_features function\n",
    "add_feature_function_list = [enron_tools.add_poi_to_ratio,enron_tools.add_poi_from_ratio,enron_tools.add_poi_interaction_ratio]\n",
    "\n",
    "## add features to data_dict\n",
    "enron_tools.add_features(add_feature_function_list,data_dict)\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "data_label = 'poi'\n",
    "features_list = enron_tools.get_features(data_dict)\n",
    "\n",
    "\n",
    "## email address does not help with prediction and causes exeception, remove\n",
    "features_list.remove('email_address')\n",
    "\n",
    "## other is not a well defined feature, remove\n",
    "#features_list.remove('other')\n",
    "\n",
    "##remove the data_label so that it can be re-added as the first feature element\n",
    "features_list.remove('poi')\n",
    "\n",
    "##reassemble feaures with the data label as the first element\n",
    "features_list = [data_label] + features_list\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "\n",
    "outliers = ['TOTAL','THE TRAVEL AGENCY IN THE PARK']\n",
    "\n",
    "enron_tools.remove_outliers(data_dict, outliers)\n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "### Continue Feature Selection and dimensionality reduction via get_k_best and pca\n",
    "\n",
    "## get k (k represents number of features) best features\n",
    "k = 10\n",
    "k_best_features = enron_tools.get_k_best(data_dict,features_list,k)\n",
    "\n",
    "print sep\n",
    "\n",
    "# assemble feature list\n",
    "my_features_list = [data_label] + list(k_best_features.feature.values)\n",
    "\n",
    "## pca\n",
    "\n",
    "'''\n",
    "pca features/data can be scaled or standardized, I experimented with both and\n",
    "ultimately opted to go with feature scaling.  Below is the code for stanardizing\n",
    "\n",
    "    std = preprocessing.StandardScaler()\n",
    "    std_pca_data = preprocessing.StandardScaler().fit_transform(data_for_pca)\n",
    "'''\n",
    "\n",
    "# remove label from features_list\n",
    "features_for_pca = features_list[1:]\n",
    "\n",
    "# extract features\n",
    "data_for_pca = featureFormat(my_dataset, features_for_pca, sort_keys = True)\n",
    "\n",
    "# scale features\n",
    "scale_pca_data = preprocessing.MinMaxScaler().fit_transform(data_for_pca)\n",
    "\n",
    "# set up PCA to explain pre-selected % of variance (perc_var)\n",
    "perc_var = .95\n",
    "pca = PCA(n_components=perc_var)\n",
    "\n",
    "# fit and transform\n",
    "pca_transform = pca.fit_transform(scale_pca_data)\n",
    "\n",
    "# Starting features and ending components\n",
    "num_features = len(features_for_pca)\n",
    "components = pca_transform.shape[1]\n",
    "print 'PCA\\n'\n",
    "print 'Explained Variance: {0}\\n Original Number of Dimensions: {1}\\n Final Dimensions: {2}\\n'.format(perc_var,num_features,components)\n",
    "print sep\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "### Task 4: Try a variety of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "## Gaussian Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "g_clf = GaussianNB()    # Provided to give you a starting point. Try a varity of classifiers.\n",
    "\n",
    "### Adaboost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "a_clf = AdaBoostClassifier(algorithm= 'SAMME')\n",
    "\n",
    "\n",
    "### Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "## Evaluate Initial Classifiers using k_best features\n",
    "\n",
    "print 'Evaluate Initial Classifiers using k_best features\\n'\n",
    "kbest_classifiers_list = [g_clf,a_clf,dt_clf]\n",
    "\n",
    "print 'Local Evaluator\\n'\n",
    "enron_evaluate.evaluate_validate(kbest_classifiers_list,my_dataset,my_features_list,scale_features=True)\n",
    "\n",
    "print sep\n",
    "\n",
    "print 'tester_scale.py evaluator\\n'\n",
    "test_classifier(g_clf,my_dataset,my_features_list, scale_features = True)\n",
    "print sep2\n",
    "test_classifier(a_clf,my_dataset,my_features_list, scale_features = True)\n",
    "print sep2\n",
    "test_classifier(dt_clf,my_dataset,my_features_list, scale_features = True)\n",
    "print sep\n",
    "\n",
    "\n",
    "## Evaluate Initial Classifiers using PCA\n",
    "## Note that feature selection is the only way to \"tune\" GaussianNB\n",
    "\n",
    "print 'Evaluate Initial Classifiers using PCA\\n'\n",
    "g_pipe = Pipeline(steps=[('pca', pca), ('gaussian', g_clf)])\n",
    "a_pipe = Pipeline(steps=[('pca', pca), ('adaboost', a_clf)])\n",
    "dt_pipe = Pipeline(steps = [('pca',pca),('decision_tree', dt_clf)])\n",
    "\n",
    "pca_classifiers_list = [g_pipe,a_pipe,dt_pipe]\n",
    "\n",
    "print 'Local Evaluator\\n'\n",
    "enron_evaluate.evaluate_validate(pca_classifiers_list,my_dataset,features_list,scale_features=True)\n",
    "\n",
    "print sep\n",
    "\n",
    "print 'tester_scale.py evaluator\\n'\n",
    "test_classifier(g_pipe,my_dataset,features_list, scale_features = True)\n",
    "print sep2\n",
    "test_classifier(a_pipe,my_dataset,features_list, scale_features = True)\n",
    "print sep2\n",
    "test_classifier(dt_pipe,my_dataset,features_list, scale_features = True)\n",
    "\n",
    "print sep\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "### extract features and labels for gridsearch optimization\n",
    "\n",
    "# data extraction using k_best features list\n",
    "data = featureFormat(my_dataset, my_features_list, sort_keys = True)\n",
    "\n",
    "# data extraction using full features list, for pipe into PCA\n",
    "#data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "\n",
    "tru, trn = targetFeatureSplit(data)\n",
    "\n",
    "## scale extracted features\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "trn = scaler.fit_transform(trn)\n",
    "\n",
    "## Inspect dimensions of piping k_best features into PCA\n",
    "# remove label from features_list\n",
    "features_for_pca = my_features_list[1:]\n",
    "\n",
    "# extract features\n",
    "data_for_pca = featureFormat(my_dataset, features_for_pca, sort_keys = True)\n",
    "\n",
    "# scale features\n",
    "scale_pca_data = preprocessing.MinMaxScaler().fit_transform(data_for_pca)\n",
    "\n",
    "# fit and transform\n",
    "pca_transform = pca.fit_transform(scale_pca_data)\n",
    "\n",
    "# Starting features and ending components\n",
    "num_features = len(features_for_pca)\n",
    "components = pca_transform.shape[1]\n",
    "print 'Dimension Reduction Piping k_best Through PCA\\n'\n",
    "print 'Explained Variance: {0}\\n Original Number of Dimensions: {1}\\n Final Dimensions: {2}\\n'.format(perc_var,num_features,components)\n",
    "print sep\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script.\n",
    "### Because of the small size of the dataset, the script uses stratified\n",
    "### shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "print \"Tune Classifiers\\n\"\n",
    "\n",
    "## Tune decision tree via gridsearch\n",
    "\n",
    "# Set up cross validator (will be used for tuning all classifiers)\n",
    "cv = cross_validation.StratifiedShuffleSplit(tru,\n",
    "                                            n_iter = 10,\n",
    "                                             random_state = 42)\n",
    "# set up estimator and pipeline, using PCA for feature selection\n",
    "estimators = [('reduce_dim', PCA()),('dec_tree',dt_clf)]\n",
    "dtclf = Pipeline(estimators)\n",
    "\n",
    "# set up paramaters dictionary\n",
    "dt_params = dict(reduce_dim__n_components=[.95,.90,.98],\n",
    "              dec_tree__criterion=(\"gini\",\"entropy\"),\n",
    "                  dec_tree__min_samples_split=[1,2,4,8,16,32],\n",
    "                   dec_tree__min_samples_leaf=[1,2,4,8,16,32],\n",
    "                   dec_tree__max_depth=[None,1,2,4,8,16,32])\n",
    "\n",
    "# set up gridsearch\n",
    "dt_grid_search = GridSearchCV(dtclf, param_grid = dt_params,\n",
    "                          scoring = 'f1', cv =cv)\n",
    "\n",
    "# pass data into into the gridsearch via fit\n",
    "dt_grid_search.fit(trn,tru)\n",
    "\n",
    "print 'Decision tree tuning\\n Steps: {0}\\n, Best Parameters: {1}\\n '.format(dtclf.steps,dt_grid_search.best_params_,dt_grid_search.best_score_)\n",
    "print sep2\n",
    "# pick a winner\n",
    "best_dtclf = dt_grid_search.best_estimator_\n",
    "\n",
    "## Tune adaboost via gridsearch\n",
    "\n",
    "# set up estimator and pipeline, using PCA for feature selection\n",
    "estimators = [('reduce_dim', PCA()),('adaboost',a_clf)]\n",
    "aclf = Pipeline(estimators)\n",
    "\n",
    "# set up paramaters dictionary\n",
    "a_params = dict(reduce_dim__n_components=[.95,.90,.98],\n",
    "                adaboost__base_estimator=[DecisionTreeClassifier(),RandomForestClassifier()],\n",
    "              adaboost__n_estimators=[5, 10, 30, 40, 50, 100, 150, 200],\n",
    "                  adaboost__learning_rate=[0.1, 0.5, 1, 1.5, 2, 2.5],\n",
    "                   adaboost__algorithm=('SAMME', 'SAMME.R'))\n",
    "\n",
    "# set up gridsearch\n",
    "a_grid_search = GridSearchCV(aclf, param_grid = a_params,\n",
    "                          scoring = 'f1', cv =cv)\n",
    "# pass data into into the gridsearch via fit\n",
    "a_grid_search.fit(trn,tru)\n",
    "\n",
    "print 'Adaboost tuning\\n Steps: {0}\\n, Best Parameters: {1}\\n '.format(aclf.steps,a_grid_search.best_params_,a_grid_search.best_score_)\n",
    "print sep2\n",
    "# pick a winner\n",
    "best_aclf = a_grid_search.best_estimator_\n",
    "\n",
    "\n",
    "## Tune gaussian via gridsearch\n",
    "\n",
    "# set up estimator and pipeline, using PCA for feature selection\n",
    "#estimators = Pipeline(steps=[('pca', PCA()), ('gaussian', g_clf)])\n",
    "#gclf = Pipeline(estimators)\n",
    "\n",
    "# set up paramaters dictionary\n",
    "#g_params = dict(reduce_dim__n_components=[.95,.90,.98])\n",
    "\n",
    "# set up gridsearch\n",
    "#g_grid_search = GridSearchCV(gclf, param_grid = g_params,\n",
    "#                          scoring = 'f1', cv =cv)\n",
    "# pass data into into the gridsearch via fit\n",
    "#g_grid_search.fit(trn,tru)\n",
    "\n",
    "#print 'Gaussian tuning\\n Steps: {0}\\n, Best Parameters: {1}\\n '.format(gclf.steps,g_grid_search.best_params_,g_grid_search.best_score_)\n",
    "#print sep2\n",
    "# pick a winner\n",
    "#best_gclf = g_grid_search.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "## Tune adaboost with best decision tree, via gridsearch \n",
    "\n",
    "# Assign the best parameters from decision tree tuning to a variable (cut and paste for now \n",
    "# there has to be a better way to do this)\n",
    "\n",
    "best_dt_params = DecisionTreeClassifier(compute_importances=None, criterion='entropy',\n",
    "            max_depth=16, max_features=None, max_leaf_nodes=None,\n",
    "            min_density=None, min_samples_leaf=1, min_samples_split=8,\n",
    "            random_state=None, splitter='best')\n",
    "\n",
    "# Set up classifier\n",
    "adt_clf = AdaBoostClassifier(best_dt_params)\n",
    "\n",
    "# Set up estimator and pipeline, using PCA for dimensitonality reduction\n",
    "estimators = [('reduce_dim', PCA()),('adaboost',adt_clf)]\n",
    "adtclf = Pipeline(estimators)\n",
    "\n",
    "# Set up parameters dictionary\n",
    "adt_params = dict(reduce_dim__n_components=[.95],\n",
    "              adaboost__n_estimators=[5, 10, 30, 40, 50, 100,150,200],\n",
    "                  adaboost__learning_rate=[0.1, 0.5, 1, 1.5, 2, 2.5],\n",
    "                   adaboost__algorithm=('SAMME', 'SAMME.R'))\n",
    "\n",
    "# Set up grid search\n",
    "adt_grid_search = GridSearchCV(adtclf, param_grid = adt_params,\n",
    "                          scoring = 'f1', cv = cv)\n",
    "\n",
    "# Pass data into the gridsearch by calling fit\n",
    "adt_grid_search.fit(trn,tru)\n",
    "\n",
    "print 'Adaboost with Tuned Decision Tree, tuning\\n Steps: {0}\\n, Best Parameters: {1}\\n '.format(adtclf.steps,adt_grid_search.best_params_,adt_grid_search.best_score_)\n",
    "print sep2\n",
    "# pick a winner\n",
    "best_adtclf = adt_grid_search.best_estimator_\n",
    "\n",
    "'''\n",
    "\n",
    "## Evaluate Tuned Classifiers\n",
    "\n",
    "print 'Evaluate Tuned Classifiers\\n'\n",
    "\n",
    "#adt_pipe = Pipeline(steps=[('pca',pca),('adaboost_dt',best_adtclf)])\n",
    "\n",
    "#best_g_pipe = Pipeline(steps=[('pca', pca), ('gaussian', best_gclf)])\n",
    "\n",
    "dt_pipe = Pipeline(steps=[('pca',pca),('dt',best_dtclf)])\n",
    "\n",
    "best_a_pipe = Pipeline(steps=[('pca',pca),('adaboost',best_aclf)])\n",
    "\n",
    "# including GaussianNB because it is \"tuned\" through feature selection\n",
    "# it will be interesting to see the result of features selected by k_best piped through PCA\n",
    "\n",
    "#print 'GaussianNB\\n'\n",
    "#test_classifier(best_g_pipe,my_dataset,my_features_list, scale_features = True, std_features= False)\n",
    "#print sep2\n",
    "\n",
    "print 'best_dt_clf\\n'\n",
    "test_classifier(dt_pipe,my_dataset,my_features_list, scale_features = True, std_features= False)\n",
    "print sep2\n",
    "\n",
    "print 'best_a_clf\\n'\n",
    "test_classifier(best_a_pipe,my_dataset,my_features_list, scale_features = True, std_features= False)\n",
    "#print sep2\n",
    "\n",
    "#print 'best_adt_clf\\n'\n",
    "#test_classifier(adt_pipe,my_dataset,my_features_list, scale_features = True, std_features= False)\n",
    "print sep\n",
    "\n",
    "### Dump your classifier, dataset, and features_list so \n",
    "### anyone can run/check your results.\n",
    "\n",
    "dump_classifier_and_data(best_aclf, my_dataset, my_features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Selection by k_best_features\n",
      "\n",
      "9 best features in descending order: ['exercised_stock_options' 'total_stock_value' 'bonus' 'salary'\n",
      " 'deferred_income' 'poi_ratio' 'long_term_incentive' 'restricted_stock'\n",
      " 'total_payments']\n",
      "\n",
      "                   feature      score  percent_nan\n",
      "0  exercised_stock_options  24.815080         29.9\n",
      "1        total_stock_value  24.182899         13.2\n",
      "2                    bonus  20.792252         43.8\n",
      "3                   salary  18.289684         34.7\n",
      "4          deferred_income  11.458477         66.7\n",
      "5                poi_ratio  10.019415         40.3\n",
      "6      long_term_incentive   9.922186         54.9\n",
      "7         restricted_stock   9.212811         24.3\n",
      "8           total_payments   8.772778         14.6\n",
      "\n",
      "##############################################################################################\n",
      "PCA\n",
      "\n",
      "Explained Variance: 0.95\n",
      " Original Number of Dimensions: 22\n",
      " Final Dimensions: 13\n",
      "\n",
      "##############################################################################################\n",
      "Evaluate Initial Classifiers using k_best features\n",
      "\n",
      "Local Evaluator\n",
      "\n",
      "clf = GaussianNB()\n",
      " Accuracy:0.860465116279\n",
      " Predicted Poi in test set:5.0\n",
      " Total Persons in test set:43\n",
      " Precision:0.4\n",
      " Recall:0.4 \n",
      " F1 Score: 0.4 \n",
      "\n",
      "clf = AdaBoostClassifier(algorithm='SAMME', base_estimator=None, learning_rate=1.0,\n",
      "          n_estimators=50, random_state=None)\n",
      " Accuracy:0.790697674419\n",
      " Predicted Poi in test set:6.0\n",
      " Total Persons in test set:43\n",
      " Precision:0.166666666667\n",
      " Recall:0.2 \n",
      " F1 Score: 0.181818181818 \n",
      "\n",
      "clf = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            random_state=None, splitter='best')\n",
      " Accuracy:0.837209302326\n",
      " Predicted Poi in test set:2.0\n",
      " Total Persons in test set:43\n",
      " Precision:0.0\n",
      " Recall:0.0 \n",
      " F1 Score: 0.0 \n",
      "\n",
      "##############################################################################################\n",
      "tester_scale.py evaluator\n",
      "\n",
      "GaussianNB()\n",
      "\tAccuracy: 0.84107\tPrecision: 0.38192\tRecall: 0.31050\tF1: 0.34253\tF2: 0.32256\n",
      "\tTotal predictions: 15000\tTrue positives:  621\tFalse positives: 1005\tFalse negatives: 1379\tTrue negatives: 11995\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-611217639d5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[0mtest_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_clf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmy_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmy_features_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0msep2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m \u001b[0mtest_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_clf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmy_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmy_features_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0msep2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[0mtest_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt_clf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmy_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmy_features_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Andrew\\Documents\\Udacity\\udacity_intromaclrn\\ud120-projects\\final_project\\tester_scale.pyc\u001b[0m in \u001b[0;36mtest_classifier\u001b[1;34m(clf, dataset, feature_list, folds, scale_features, std_features)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m### fit the classifier using training set, and test on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruth\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Andrew\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Andrew\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                 sample_weight)\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Andrew\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.pyc\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_discrete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Andrew\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.pyc\u001b[0m in \u001b[0;36m_boost_discrete\u001b[1;34m(self, iboost, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    537\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 539\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Andrew\\Anaconda\\lib\\site-packages\\sklearn\\tree\\tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_classification\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Andrew\\Anaconda\\lib\\site-packages\\numpy\\lib\\function_base.pyc\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(a, order)\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m     \"\"\"\n\u001b[1;32m--> 881\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;31m# Basic operations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester_scale import test_classifier, dump_classifier_and_data\n",
    "\n",
    "#additional imports\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import enron_tools\n",
    "import enron_evaluate\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "import enron_evaluate\n",
    "\n",
    "sep = '##############################################################################################'\n",
    "sep2 = '++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++'\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### create list of functions for use as argument to add_features function\n",
    "add_feature_function_list = [enron_tools.add_poi_to_ratio,enron_tools.add_poi_from_ratio,enron_tools.add_poi_interaction_ratio]\n",
    "\n",
    "## add features to data_dict\n",
    "enron_tools.add_features(add_feature_function_list,data_dict)\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "data_label = 'poi'\n",
    "features_list = enron_tools.get_features(data_dict)\n",
    "\n",
    "\n",
    "## email address does not help with prediction and causes exeception, remove\n",
    "features_list.remove('email_address')\n",
    "\n",
    "## other is not a well defined feature, remove\n",
    "#features_list.remove('other')\n",
    "\n",
    "##remove the data_label so that it can be re-added as the first feature element\n",
    "features_list.remove('poi')\n",
    "\n",
    "##reassemble feaures with the data label as the first element\n",
    "features_list = [data_label] + features_list\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "\n",
    "outliers = ['TOTAL','THE TRAVEL AGENCY IN THE PARK']\n",
    "\n",
    "enron_tools.remove_outliers(data_dict, outliers)\n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "### Continue Feature Selection and dimensionality reduction via get_k_best and pca\n",
    "\n",
    "## get k (k represents number of features) best features\n",
    "k = 9\n",
    "k_best_features = enron_tools.get_k_best(data_dict,features_list,k)\n",
    "\n",
    "print sep\n",
    "\n",
    "# assemble feature list\n",
    "my_features_list = [data_label] + list(k_best_features.feature.values)\n",
    "\n",
    "## pca\n",
    "\n",
    "'''\n",
    "pca features/data can be scaled or standardized, I experimented with both and\n",
    "ultimately opted to go with feature scaling.  Below is the code for stanardizing\n",
    "\n",
    "    std = preprocessing.StandardScaler()\n",
    "    std_pca_data = preprocessing.StandardScaler().fit_transform(data_for_pca)\n",
    "'''\n",
    "\n",
    "# remove label from features_list\n",
    "features_for_pca = features_list[1:]\n",
    "\n",
    "# extract features\n",
    "data_for_pca = featureFormat(my_dataset, features_for_pca, sort_keys = True)\n",
    "\n",
    "# scale features\n",
    "scale_pca_data = preprocessing.MinMaxScaler().fit_transform(data_for_pca)\n",
    "\n",
    "# set up PCA to explain pre-selected % of variance (perc_var)\n",
    "perc_var = .95\n",
    "pca = PCA(n_components=perc_var)\n",
    "\n",
    "# fit and transform\n",
    "pca_transform = pca.fit_transform(scale_pca_data)\n",
    "\n",
    "# Starting features and ending components\n",
    "num_features = len(features_for_pca)\n",
    "components = pca_transform.shape[1]\n",
    "print 'PCA\\n'\n",
    "print 'Explained Variance: {0}\\n Original Number of Dimensions: {1}\\n Final Dimensions: {2}\\n'.format(perc_var,num_features,components)\n",
    "print sep\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "### Task 4: Try a variety of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "## Gaussian Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "g_clf = GaussianNB()    # Provided to give you a starting point. Try a varity of classifiers.\n",
    "\n",
    "### Adaboost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "a_clf = AdaBoostClassifier(algorithm= 'SAMME')\n",
    "\n",
    "\n",
    "### Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "## Evaluate Initial Classifiers using k_best features\n",
    "\n",
    "print 'Evaluate Initial Classifiers using k_best features\\n'\n",
    "kbest_classifiers_list = [g_clf,a_clf,dt_clf]\n",
    "\n",
    "print 'Local Evaluator\\n'\n",
    "enron_evaluate.evaluate_validate(kbest_classifiers_list,my_dataset,my_features_list,scale_features=True)\n",
    "\n",
    "print sep\n",
    "\n",
    "print 'tester_scale.py evaluator\\n'\n",
    "test_classifier(g_clf,my_dataset,my_features_list, scale_features = True)\n",
    "print sep2\n",
    "test_classifier(a_clf,my_dataset,my_features_list, scale_features = True)\n",
    "print sep2\n",
    "test_classifier(dt_clf,my_dataset,my_features_list, scale_features = True)\n",
    "print sep\n",
    "\n",
    "\n",
    "## Evaluate Initial Classifiers using PCA\n",
    "## Note that feature selection is the only way to \"tune\" GaussianNB\n",
    "\n",
    "print 'Evaluate Initial Classifiers using PCA\\n'\n",
    "g_pipe = Pipeline(steps=[('pca', pca), ('gaussian', g_clf)])\n",
    "a_pipe = Pipeline(steps=[('pca', pca), ('adaboost', a_clf)])\n",
    "dt_pipe = Pipeline(steps = [('pca',pca),('decision_tree', dt_clf)])\n",
    "\n",
    "pca_classifiers_list = [g_pipe,a_pipe,dt_pipe]\n",
    "\n",
    "print 'Local Evaluator\\n'\n",
    "enron_evaluate.evaluate_validate(pca_classifiers_list,my_dataset,features_list,scale_features=True)\n",
    "\n",
    "print sep\n",
    "\n",
    "print 'tester_scale.py evaluator\\n'\n",
    "test_classifier(g_pipe,my_dataset,features_list, scale_features = True)\n",
    "print sep2\n",
    "test_classifier(a_pipe,my_dataset,features_list, scale_features = True)\n",
    "print sep2\n",
    "test_classifier(dt_pipe,my_dataset,features_list, scale_features = True)\n",
    "\n",
    "print sep\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "### extract features and labels for gridsearch optimization\n",
    "\n",
    "# data extraction using k_best features list\n",
    "data = featureFormat(my_dataset, my_features_list, sort_keys = True)\n",
    "\n",
    "# data extraction using full features list, for pipe into PCA\n",
    "#data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "\n",
    "tru, trn = targetFeatureSplit(data)\n",
    "\n",
    "## scale extracted features\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "trn = scaler.fit_transform(trn)\n",
    "\n",
    "## Inspect dimensions of piping k_best features into PCA\n",
    "# remove label from features_list\n",
    "features_for_pca = my_features_list[1:]\n",
    "\n",
    "# extract features\n",
    "data_for_pca = featureFormat(my_dataset, features_for_pca, sort_keys = True)\n",
    "\n",
    "# scale features\n",
    "scale_pca_data = preprocessing.MinMaxScaler().fit_transform(data_for_pca)\n",
    "\n",
    "# fit and transform\n",
    "pca_transform = pca.fit_transform(scale_pca_data)\n",
    "\n",
    "# Starting features and ending components\n",
    "num_features = len(features_for_pca)\n",
    "components = pca_transform.shape[1]\n",
    "print 'Dimension Reduction Piping k_best Through PCA\\n'\n",
    "print 'Explained Variance: {0}\\n Original Number of Dimensions: {1}\\n Final Dimensions: {2}\\n'.format(perc_var,num_features,components)\n",
    "print sep\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script.\n",
    "### Because of the small size of the dataset, the script uses stratified\n",
    "### shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "print \"Tune Classifiers\\n\"\n",
    "\n",
    "## Tune decision tree via gridsearch\n",
    "\n",
    "# Set up cross validator (will be used for tuning all classifiers)\n",
    "cv = cross_validation.StratifiedShuffleSplit(tru,\n",
    "                                            n_iter = 10,\n",
    "                                             random_state = 42)\n",
    "# set up estimator and pipeline, using PCA for feature selection\n",
    "estimators = [('reduce_dim', PCA()),('dec_tree',dt_clf)]\n",
    "dtclf = Pipeline(estimators)\n",
    "\n",
    "# set up paramaters dictionary\n",
    "dt_params = dict(reduce_dim__n_components=[.95],\n",
    "              dec_tree__criterion=(\"gini\",\"entropy\"),\n",
    "                  dec_tree__min_samples_split=[1,2,4,8,16,32],\n",
    "                   dec_tree__min_samples_leaf=[1,2,4,8,16,32],\n",
    "                   dec_tree__max_depth=[None,1,2,4,8,16,32])\n",
    "\n",
    "# set up gridsearch\n",
    "dt_grid_search = GridSearchCV(dtclf, param_grid = dt_params,\n",
    "                          scoring = 'f1', cv =cv)\n",
    "\n",
    "# pass data into into the gridsearch via fit\n",
    "dt_grid_search.fit(trn,tru)\n",
    "\n",
    "print 'Decision tree tuning\\n Steps: {0}\\n, Best Parameters: {1}\\n '.format(dtclf.steps,dt_grid_search.best_params_,dt_grid_search.best_score_)\n",
    "print sep2\n",
    "# pick a winner\n",
    "best_dtclf = dt_grid_search.best_estimator_\n",
    "\n",
    "## Tune adaboost via gridsearch\n",
    "\n",
    "# set up estimator and pipeline, using PCA for feature selection\n",
    "estimators = [('reduce_dim', PCA()),('adaboost',a_clf)]\n",
    "aclf = Pipeline(estimators)\n",
    "\n",
    "# set up paramaters dictionary\n",
    "a_params = dict(reduce_dim__n_components=[.95],\n",
    "                adaboost__base_estimator=[DecisionTreeClassifier(),GaussianNB()],\n",
    "              adaboost__n_estimators=[5, 10, 30, 40, 50, 100, 150, 200],\n",
    "                  adaboost__learning_rate=[0.1, 0.5, 1, 1.5, 2, 2.5],\n",
    "                   adaboost__algorithm=('SAMME', 'SAMME.R'))\n",
    "\n",
    "# set up gridsearch\n",
    "a_grid_search = GridSearchCV(aclf, param_grid = a_params,\n",
    "                          scoring = 'f1', cv =cv)\n",
    "# pass data into into the gridsearch via fit\n",
    "a_grid_search.fit(trn,tru)\n",
    "\n",
    "print 'Adaboost tuning\\n Steps: {0}\\n, Best Parameters: {1}\\n '.format(aclf.steps,a_grid_search.best_params_,a_grid_search.best_score_)\n",
    "print sep2\n",
    "# pick a winner\n",
    "best_aclf = a_grid_search.best_estimator_\n",
    "\n",
    "'''\n",
    "## Tune adaboost with best decision tree, via gridsearch \n",
    "\n",
    "# Assign the best parameters from decision tree tuning to a variable (cut and paste for now \n",
    "# there has to be a better way to do this)\n",
    "\n",
    "best_dt_params = DecisionTreeClassifier(compute_importances=None, criterion='entropy',\n",
    "            max_depth=16, max_features=None, max_leaf_nodes=None,\n",
    "            min_density=None, min_samples_leaf=1, min_samples_split=8,\n",
    "            random_state=None, splitter='best')\n",
    "\n",
    "# Set up classifier\n",
    "adt_clf = AdaBoostClassifier(best_dt_params)\n",
    "\n",
    "# Set up estimator and pipeline, using PCA for dimensitonality reduction\n",
    "estimators = [('reduce_dim', PCA()),('adaboost',adt_clf)]\n",
    "adtclf = Pipeline(estimators)\n",
    "\n",
    "# Set up parameters dictionary\n",
    "adt_params = dict(reduce_dim__n_components=[.95],\n",
    "              adaboost__n_estimators=[5, 10, 30, 40, 50, 100,150,200],\n",
    "                  adaboost__learning_rate=[0.1, 0.5, 1, 1.5, 2, 2.5],\n",
    "                   adaboost__algorithm=('SAMME', 'SAMME.R'))\n",
    "\n",
    "# Set up grid search\n",
    "adt_grid_search = GridSearchCV(adtclf, param_grid = adt_params,\n",
    "                          scoring = 'f1', cv = cv)\n",
    "\n",
    "# Pass data into the gridsearch by calling fit\n",
    "adt_grid_search.fit(trn,tru)\n",
    "\n",
    "print 'Adaboost with Tuned Decision Tree, tuning\\n Steps: {0}\\n, Best Parameters: {1}\\n '.format(adtclf.steps,adt_grid_search.best_params_,adt_grid_search.best_score_)\n",
    "print sep2\n",
    "# pick a winner\n",
    "best_adtclf = adt_grid_search.best_estimator_\n",
    "\n",
    "'''\n",
    "\n",
    "## Evaluate Tuned Classifiers\n",
    "\n",
    "print 'Evaluate Tuned Classifiers\\n'\n",
    "\n",
    "#adt_pipe = Pipeline(steps=[('pca',pca),('adaboost_dt',best_adtclf)])\n",
    "\n",
    "dt_pipe = Pipeline(steps=[('pca',pca),('dt',best_dtclf)])\n",
    "\n",
    "best_a_pipe = Pipeline(steps=[('pca',pca),('adaboost',best_aclf)])\n",
    "\n",
    "# including GaussianNB because it is \"tuned\" through feature selection\n",
    "# it will be interesting to see the result of features selected by k_best piped through PCA\n",
    "\n",
    "print 'GaussianNB\\n'\n",
    "test_classifier(g_pipe,my_dataset,my_features_list, scale_features = True, std_features= False)\n",
    "print sep2\n",
    "\n",
    "print 'best_dt_clf\\n'\n",
    "test_classifier(dt_pipe,my_dataset,my_features_list, scale_features = True, std_features= False)\n",
    "print sep2\n",
    "\n",
    "print 'best_a_clf\\n'\n",
    "test_classifier(best_a_pipe,my_dataset,my_features_list, scale_features = True, std_features= False)\n",
    "#print sep2\n",
    "\n",
    "#print 'best_adt_clf\\n'\n",
    "#test_classifier(adt_pipe,my_dataset,my_features_list, scale_features = True, std_features= False)\n",
    "print sep\n",
    "\n",
    "### Dump your classifier, dataset, and features_list so \n",
    "### anyone can run/check your results.\n",
    "\n",
    "dump_classifier_and_data(best_aclf, my_dataset, my_features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adaboost__algorithm': ('SAMME', 'SAMME.R'),\n",
       " 'adaboost__base_estimator': [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              random_state=None, splitter='best'),\n",
       "  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False)],\n",
       " 'adaboost__learning_rate': [0.1, 0.5, 1, 1.5, 2, 2.5],\n",
       " 'adaboost__n_estimators': [5, 10, 30, 40, 50, 100, 150, 200],\n",
       " 'reduce_dim__n_components': [0.95]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc_var = .95\n",
    "dict(reduce_dim__n_components=[perc_var],\n",
    "              adaboost__base_estimator=[DecisionTreeClassifier(),RandomForestClassifier()],\n",
    "              adaboost__n_estimators=[5, 10, 30, 40, 50, 100, 150, 200],\n",
    "                  adaboost__learning_rate=[0.1, 0.5, 1, 1.5, 2, 2.5],\n",
    "                   adaboost__algorithm=('SAMME', 'SAMME.R'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
