{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Selection by k_best_features\n",
      "\n",
      "10 best features in descending order: ['exercised_stock_options' 'total_stock_value' 'bonus' 'salary'\n",
      " 'deferred_income' 'poi_ratio' 'long_term_incentive' 'restricted_stock'\n",
      " 'total_payments' 'shared_receipt_with_poi']\n",
      "\n",
      "                   feature      score  percent_nan\n",
      "0  exercised_stock_options  24.815080         29.9\n",
      "1        total_stock_value  24.182899         13.2\n",
      "2                    bonus  20.792252         43.8\n",
      "3                   salary  18.289684         34.7\n",
      "4          deferred_income  11.458477         66.7\n",
      "5                poi_ratio  10.019415         40.3\n",
      "6      long_term_incentive   9.922186         54.9\n",
      "7         restricted_stock   9.212811         24.3\n",
      "8           total_payments   8.772778         14.6\n",
      "9  shared_receipt_with_poi   8.589421         40.3\n",
      "\n",
      "##############################################################################################\n",
      "PCA\n",
      "\n",
      "Explained Variance: 0.95\n",
      " Original Number of Dimensions: 22\n",
      " Final Dimensions: 13\n",
      "\n",
      "##############################################################################################\n",
      "Evaluate Initial Classifiers using k_best features\n",
      "\n",
      "Local Evaluator\n",
      "\n",
      "clf = GaussianNB()\n",
      " Accuracy:0.883720930233\n",
      " Predicted Poi in test set:6.0\n",
      " Total Persons in test set:43\n",
      " Precision:0.5\n",
      " Recall:0.6 \n",
      " F1 Score: 0.545454545455 \n",
      "\n",
      "clf = AdaBoostClassifier(algorithm='SAMME', base_estimator=None, learning_rate=1.0,\n",
      "          n_estimators=50, random_state=None)\n",
      " Accuracy:0.813953488372\n",
      " Predicted Poi in test set:5.0\n",
      " Total Persons in test set:43\n",
      " Precision:0.2\n",
      " Recall:0.2 \n",
      " F1 Score: 0.2 \n",
      "\n",
      "clf = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            random_state=None, splitter='best')\n",
      " Accuracy:0.837209302326\n",
      " Predicted Poi in test set:2.0\n",
      " Total Persons in test set:43\n",
      " Precision:0.0\n",
      " Recall:0.0 \n",
      " F1 Score: 0.0 \n",
      "\n",
      "##############################################################################################\n",
      "tester_scale.py evaluator\n",
      "\n",
      "GaussianNB()\n",
      "\tAccuracy: 0.83533\tPrecision: 0.36225\tRecall: 0.30900\tF1: 0.33351\tF2: 0.31836\n",
      "\tTotal predictions: 15000\tTrue positives:  618\tFalse positives: 1088\tFalse negatives: 1382\tTrue negatives: 11912\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "AdaBoostClassifier(algorithm='SAMME', base_estimator=None, learning_rate=1.0,\n",
      "          n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.83613\tPrecision: 0.32220\tRecall: 0.20750\tF1: 0.25243\tF2: 0.22341\n",
      "\tTotal predictions: 15000\tTrue positives:  415\tFalse positives:  873\tFalse negatives: 1585\tTrue negatives: 12127\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            random_state=None, splitter='best')\n",
      "\tAccuracy: 0.80233\tPrecision: 0.24239\tRecall: 0.22700\tF1: 0.23444\tF2: 0.22992\n",
      "\tTotal predictions: 15000\tTrue positives:  454\tFalse positives: 1419\tFalse negatives: 1546\tTrue negatives: 11581\n",
      "\n",
      "##############################################################################################\n",
      "Evaluate Initial Classifiers using PCA\n",
      "\n",
      "Local Evaluator\n",
      "\n",
      "clf = Pipeline(steps=[('pca', PCA(copy=True, n_components=0.95, whiten=False)), ('gaussian', GaussianNB())])\n",
      " Accuracy:0.860465116279\n",
      " Predicted Poi in test set:3.0\n",
      " Total Persons in test set:43\n",
      " Precision:0.333333333333\n",
      " Recall:0.2 \n",
      " F1 Score: 0.25 \n",
      "\n",
      "clf = Pipeline(steps=[('pca', PCA(copy=True, n_components=0.95, whiten=False)), ('adaboost', AdaBoostClassifier(algorithm='SAMME', base_estimator=None, learning_rate=1.0,\n",
      "          n_estimators=50, random_state=None))])\n",
      " Accuracy:0.767441860465\n",
      " Predicted Poi in test set:7.0\n",
      " Total Persons in test set:43\n",
      " Precision:0.142857142857\n",
      " Recall:0.2 \n",
      " F1 Score: 0.166666666667 \n",
      "\n",
      "clf = Pipeline(steps=[('pca', PCA(copy=True, n_components=0.95, whiten=False)), ('decision_tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            random_state=None, splitter='best'))])\n",
      " Accuracy:0.744186046512\n",
      " Predicted Poi in test set:10.0\n",
      " Total Persons in test set:43\n",
      " Precision:0.2\n",
      " Recall:0.4 \n",
      " F1 Score: 0.266666666667 \n",
      "\n",
      "##############################################################################################\n",
      "tester_scale.py evaluator\n",
      "\n",
      "Pipeline(steps=[('pca', PCA(copy=True, n_components=0.95, whiten=False)), ('gaussian', GaussianNB())])\n",
      "\tAccuracy: 0.82573\tPrecision: 0.35039\tRecall: 0.35950\tF1: 0.35489\tF2: 0.35764\n",
      "\tTotal predictions: 15000\tTrue positives:  719\tFalse positives: 1333\tFalse negatives: 1281\tTrue negatives: 11667\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Pipeline(steps=[('pca', PCA(copy=True, n_components=0.95, whiten=False)), ('adaboost', AdaBoostClassifier(algorithm='SAMME', base_estimator=None, learning_rate=1.0,\n",
      "          n_estimators=50, random_state=None))])\n",
      "\tAccuracy: 0.84780\tPrecision: 0.38919\tRecall: 0.24850\tF1: 0.30333\tF2: 0.26787\n",
      "\tTotal predictions: 15000\tTrue positives:  497\tFalse positives:  780\tFalse negatives: 1503\tTrue negatives: 12220\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Pipeline(steps=[('pca', PCA(copy=True, n_components=0.95, whiten=False)), ('decision_tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.81193\tPrecision: 0.29003\tRecall: 0.28350\tF1: 0.28673\tF2: 0.28478\n",
      "\tTotal predictions: 15000\tTrue positives:  567\tFalse positives: 1388\tFalse negatives: 1433\tTrue negatives: 11612\n",
      "\n",
      "##############################################################################################\n",
      "Dimension Reduction Piping k_best Through PCA\n",
      "\n",
      "Explained Variance: 0.95\n",
      " Original Number of Dimensions: 10\n",
      " Final Dimensions: 6\n",
      "\n",
      "##############################################################################################\n",
      "Tune Classifiers\n",
      "\n",
      "Decision tree tuning\n",
      " Steps: [('reduce_dim', PCA(copy=True, n_components=None, whiten=False)), ('dec_tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            random_state=None, splitter='best'))]\n",
      ", Best Parameters: {'dec_tree__max_depth': 4, 'dec_tree__criterion': 'gini', 'dec_tree__min_samples_leaf': 1, 'reduce_dim__n_components': 0.95, 'dec_tree__min_samples_split': 2}\n",
      " \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Adaboost tuning\n",
      " Steps: [('reduce_dim', PCA(copy=True, n_components=None, whiten=False)), ('adaboost', AdaBoostClassifier(algorithm='SAMME', base_estimator=None, learning_rate=1.0,\n",
      "          n_estimators=50, random_state=None))]\n",
      ", Best Parameters: {'reduce_dim__n_components': 0.95, 'adaboost__algorithm': 'SAMME.R', 'adaboost__n_estimators': 5, 'adaboost__learning_rate': 2}\n",
      " \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Evaluate Tuned Classifiers\n",
      "\n",
      "GaussianNB\n",
      "\n",
      "Pipeline(steps=[('pca', PCA(copy=True, n_components=0.95, whiten=False)), ('gaussian', GaussianNB())])\n",
      "\tAccuracy: 0.85200\tPrecision: 0.43799\tRecall: 0.38850\tF1: 0.41176\tF2: 0.39748\n",
      "\tTotal predictions: 15000\tTrue positives:  777\tFalse positives:  997\tFalse negatives: 1223\tTrue negatives: 12003\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "best_dt_clf\n",
      "\n",
      "Pipeline(steps=[('pca', PCA(copy=True, n_components=0.95, whiten=False)), ('dt', Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=0.95, whiten=False)), ('dec_tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            random_state=None, splitter='best'))]))])\n",
      "\tAccuracy: 0.84693\tPrecision: 0.40704\tRecall: 0.32400\tF1: 0.36080\tF2: 0.33778\n",
      "\tTotal predictions: 15000\tTrue positives:  648\tFalse positives:  944\tFalse negatives: 1352\tTrue negatives: 12056\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "best_a_clf\n",
      "\n",
      "Pipeline(steps=[('pca', PCA(copy=True, n_components=0.95, whiten=False)), ('adaboost', Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=0.95, whiten=False)), ('adaboost', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=2,\n",
      "          n_estimators=5, random_state=None))]))])\n",
      "\tAccuracy: 0.89973\tPrecision: 0.73134\tRecall: 0.39200\tF1: 0.51042\tF2: 0.43210\n",
      "\tTotal predictions: 15000\tTrue positives:  784\tFalse positives:  288\tFalse negatives: 1216\tTrue negatives: 12712\n",
      "\n",
      "##############################################################################################\n",
      "Best adaboost classifier with scaled pipeline run through tester.py\n",
      "\n",
      "Pipeline(steps=[('scale_features', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, n_components=0.95, whiten=False)), ('adaboost', Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=0.95, whiten=False)), ('adaboost', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=2,\n",
      "          n_estimators=5, random_state=None))]))])\n",
      "\tAccuracy: 0.89220\tPrecision: 0.67914\tRecall: 0.36300\tF1: 0.47312\tF2: 0.40026\n",
      "\tTotal predictions: 15000\tTrue positives:  726\tFalse positives:  343\tFalse negatives: 1274\tTrue negatives: 12657\n",
      "\n",
      "##############################################################################################\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester_scale import test_classifier, dump_classifier_and_data\n",
    "\n",
    "#additional imports\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import enron_tools\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "import enron_evaluate\n",
    "\n",
    "sep = '##############################################################################################'\n",
    "sep2 = '++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++'\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### create list of functions for use as argument to add_features function\n",
    "add_feature_function_list = [enron_tools.add_poi_to_ratio,enron_tools.add_poi_from_ratio,enron_tools.add_poi_interaction_ratio]\n",
    "\n",
    "## add features to data_dict\n",
    "enron_tools.add_features(add_feature_function_list,data_dict)\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "data_label = 'poi'\n",
    "features_list = enron_tools.get_features(data_dict)\n",
    "\n",
    "\n",
    "## email address does not help with prediction and causes exeception, remove\n",
    "features_list.remove('email_address')\n",
    "\n",
    "## other is not a well defined feature, remove\n",
    "#features_list.remove('other')\n",
    "\n",
    "##remove the data_label so that it can be re-added as the first feature element\n",
    "features_list.remove('poi')\n",
    "\n",
    "##reassemble feaures with the data label as the first element\n",
    "features_list = [data_label] + features_list\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "\n",
    "outliers = ['TOTAL','THE TRAVEL AGENCY IN THE PARK']\n",
    "\n",
    "enron_tools.remove_outliers(data_dict, outliers)\n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "### Continue Feature Selection and dimensionality reduction via get_k_best and pca\n",
    "\n",
    "## get k (k represents number of features) best features\n",
    "k = 10\n",
    "k_best_features = enron_tools.get_k_best(data_dict,features_list,k)\n",
    "\n",
    "print sep\n",
    "\n",
    "# assemble feature list\n",
    "my_features_list = [data_label] + list(k_best_features.feature.values)\n",
    "\n",
    "## pca\n",
    "\n",
    "'''\n",
    "pca features/data can be scaled or standardized, I experimented with both and\n",
    "ultimately opted to go with feature scaling.  Below is the code for stanardizing\n",
    "\n",
    "    std = preprocessing.StandardScaler()\n",
    "    std_pca_data = preprocessing.StandardScaler().fit_transform(data_for_pca)\n",
    "'''\n",
    "\n",
    "# remove label from features_list\n",
    "features_for_pca = features_list[1:]\n",
    "\n",
    "# extract features\n",
    "data_for_pca = featureFormat(my_dataset, features_for_pca, sort_keys = True)\n",
    "\n",
    "# scale features\n",
    "scale_pca_data = preprocessing.MinMaxScaler().fit_transform(data_for_pca)\n",
    "\n",
    "# set up PCA to explain pre-selected % of variance (perc_var)\n",
    "perc_var = .95\n",
    "pca = PCA(n_components=perc_var)\n",
    "\n",
    "# fit and transform\n",
    "pca_transform = pca.fit_transform(scale_pca_data)\n",
    "\n",
    "# Starting features and ending components\n",
    "num_features = len(features_for_pca)\n",
    "components = pca_transform.shape[1]\n",
    "print 'PCA\\n'\n",
    "print 'Explained Variance: {0}\\n Original Number of Dimensions: {1}\\n Final Dimensions: {2}\\n'.format(perc_var,num_features,components)\n",
    "print sep\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "### Task 4: Try a variety of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "## Gaussian Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "g_clf = GaussianNB()    # Provided to give you a starting point. Try a varity of classifiers.\n",
    "\n",
    "### Adaboost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "a_clf = AdaBoostClassifier(algorithm= 'SAMME')\n",
    "\n",
    "\n",
    "### Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "## Evaluate Initial Classifiers using k_best features\n",
    "\n",
    "print 'Evaluate Initial Classifiers using k_best features\\n'\n",
    "kbest_classifiers_list = [g_clf,a_clf,dt_clf]\n",
    "\n",
    "print 'Local Evaluator\\n'\n",
    "enron_evaluate.evaluate_validate(kbest_classifiers_list,my_dataset,my_features_list,scale_features=True)\n",
    "\n",
    "print sep\n",
    "\n",
    "print 'tester_scale.py evaluator\\n'\n",
    "test_classifier(g_clf,my_dataset,my_features_list, scale_features = True)\n",
    "print sep2\n",
    "test_classifier(a_clf,my_dataset,my_features_list, scale_features = True)\n",
    "print sep2\n",
    "test_classifier(dt_clf,my_dataset,my_features_list, scale_features = True)\n",
    "print sep\n",
    "\n",
    "\n",
    "## Evaluate Initial Classifiers using PCA\n",
    "## Note that feature selection is the only way to \"tune\" GaussianNB\n",
    "\n",
    "print 'Evaluate Initial Classifiers using PCA\\n'\n",
    "g_pipe = Pipeline(steps=[('pca', pca), ('gaussian', g_clf)])\n",
    "a_pipe = Pipeline(steps=[('pca', pca), ('adaboost', a_clf)])\n",
    "dt_pipe = Pipeline(steps = [('pca',pca),('decision_tree', dt_clf)])\n",
    "\n",
    "pca_classifiers_list = [g_pipe,a_pipe,dt_pipe]\n",
    "\n",
    "print 'Local Evaluator\\n'\n",
    "enron_evaluate.evaluate_validate(pca_classifiers_list,my_dataset,features_list,scale_features=True)\n",
    "\n",
    "print sep\n",
    "\n",
    "print 'tester_scale.py evaluator\\n'\n",
    "test_classifier(g_pipe,my_dataset,features_list, scale_features = True)\n",
    "print sep2\n",
    "test_classifier(a_pipe,my_dataset,features_list, scale_features = True)\n",
    "print sep2\n",
    "test_classifier(dt_pipe,my_dataset,features_list, scale_features = True)\n",
    "\n",
    "print sep\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "### extract features and labels for gridsearch optimization\n",
    "\n",
    "# data extraction using k_best features list\n",
    "data = featureFormat(my_dataset, my_features_list, sort_keys = True)\n",
    "\n",
    "# data extraction using full features list, for pipe into PCA\n",
    "#data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "\n",
    "tru, trn = targetFeatureSplit(data)\n",
    "\n",
    "## scale extracted features\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "trn = scaler.fit_transform(trn)\n",
    "\n",
    "## Inspect dimensions of piping k_best features into PCA\n",
    "# remove label from features_list\n",
    "features_for_pca = my_features_list[1:]\n",
    "\n",
    "# extract features\n",
    "data_for_pca = featureFormat(my_dataset, features_for_pca, sort_keys = True)\n",
    "\n",
    "# scale features\n",
    "scale_pca_data = preprocessing.MinMaxScaler().fit_transform(data_for_pca)\n",
    "\n",
    "# fit and transform\n",
    "pca_transform = pca.fit_transform(scale_pca_data)\n",
    "\n",
    "# Starting features and ending components\n",
    "num_features = len(features_for_pca)\n",
    "components = pca_transform.shape[1]\n",
    "print 'Dimension Reduction Piping k_best Through PCA\\n'\n",
    "print 'Explained Variance: {0}\\n Original Number of Dimensions: {1}\\n Final Dimensions: {2}\\n'.format(perc_var,num_features,components)\n",
    "print sep\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script.\n",
    "### Because of the small size of the dataset, the script uses stratified\n",
    "### shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "print \"Tune Classifiers\\n\"\n",
    "\n",
    "## Tune decision tree via gridsearch\n",
    "\n",
    "# Set up cross validator (will be used for tuning all classifiers)\n",
    "cv = cross_validation.StratifiedShuffleSplit(tru,\n",
    "                                            n_iter = 10,\n",
    "                                             random_state = 42)\n",
    "# set up estimator and pipeline, using PCA for feature selection\n",
    "estimators = [('reduce_dim', PCA()),('dec_tree',dt_clf)]\n",
    "dtclf = Pipeline(estimators)\n",
    "\n",
    "# set up paramaters dictionary\n",
    "dt_params = dict(reduce_dim__n_components=[perc_var],\n",
    "              dec_tree__criterion=(\"gini\",\"entropy\"),\n",
    "                  dec_tree__min_samples_split=[1,2,4,8,16,32],\n",
    "                   dec_tree__min_samples_leaf=[1,2,4,8,16,32],\n",
    "                   dec_tree__max_depth=[None,1,2,4,8,16,32])\n",
    "\n",
    "# set up gridsearch\n",
    "dt_grid_search = GridSearchCV(dtclf, param_grid = dt_params,\n",
    "                          scoring = 'f1', cv =cv)\n",
    "\n",
    "# pass data into into the gridsearch via fit\n",
    "dt_grid_search.fit(trn,tru)\n",
    "\n",
    "print 'Decision tree tuning\\n Steps: {0}\\n, Best Parameters: {1}\\n '.format(dtclf.steps,dt_grid_search.best_params_,dt_grid_search.best_score_)\n",
    "print sep2\n",
    "# pick a winner\n",
    "best_dtclf = dt_grid_search.best_estimator_\n",
    "\n",
    "## Tune adaboost via gridsearch\n",
    "\n",
    "# set up estimator and pipeline, using PCA for feature selection\n",
    "estimators = [('reduce_dim', PCA()),('adaboost',a_clf)]\n",
    "aclf = Pipeline(estimators)\n",
    "\n",
    "# set up paramaters dictionary\n",
    "a_params = dict(reduce_dim__n_components=[perc_var],\n",
    "              adaboost__n_estimators=[5, 10, 30, 40, 50, 100, 150, 200],\n",
    "                  adaboost__learning_rate=[0.1, 0.5, 1, 1.5, 2, 2.5],\n",
    "                   adaboost__algorithm=('SAMME', 'SAMME.R'))\n",
    "\n",
    "# set up gridsearch\n",
    "a_grid_search = GridSearchCV(aclf, param_grid = a_params,\n",
    "                          scoring = 'f1', cv =cv)\n",
    "# pass data into into the gridsearch via fit\n",
    "a_grid_search.fit(trn,tru)\n",
    "\n",
    "print 'Adaboost tuning\\n Steps: {0}\\n, Best Parameters: {1}\\n '.format(aclf.steps,a_grid_search.best_params_,a_grid_search.best_score_)\n",
    "print sep2\n",
    "# pick a winner\n",
    "best_aclf = a_grid_search.best_estimator_\n",
    "\n",
    "'''\n",
    "## Tune adaboost with best decision tree, via gridsearch \n",
    "\n",
    "# Assign the best parameters from decision tree tuning to a variable (cut and paste for now \n",
    "# there has to be a better way to do this)\n",
    "\n",
    "best_dt_params = DecisionTreeClassifier(compute_importances=None, criterion='entropy',\n",
    "            max_depth=16, max_features=None, max_leaf_nodes=None,\n",
    "            min_density=None, min_samples_leaf=1, min_samples_split=8,\n",
    "            random_state=None, splitter='best')\n",
    "\n",
    "# Set up classifier\n",
    "adt_clf = AdaBoostClassifier(best_dt_params)\n",
    "\n",
    "# Set up estimator and pipeline, using PCA for dimensitonality reduction\n",
    "estimators = [('reduce_dim', PCA()),('adaboost',adt_clf)]\n",
    "adtclf = Pipeline(estimators)\n",
    "\n",
    "# Set up parameters dictionary\n",
    "adt_params = dict(reduce_dim__n_components=[.95],\n",
    "              adaboost__n_estimators=[5, 10, 30, 40, 50, 100,150,200],\n",
    "                  adaboost__learning_rate=[0.1, 0.5, 1, 1.5, 2, 2.5],\n",
    "                   adaboost__algorithm=('SAMME', 'SAMME.R'))\n",
    "\n",
    "# Set up grid search\n",
    "adt_grid_search = GridSearchCV(adtclf, param_grid = adt_params,\n",
    "                          scoring = 'f1', cv = cv)\n",
    "\n",
    "# Pass data into the gridsearch by calling fit\n",
    "adt_grid_search.fit(trn,tru)\n",
    "\n",
    "print 'Adaboost with Tuned Decision Tree, tuning\\n Steps: {0}\\n, Best Parameters: {1}\\n '.format(adtclf.steps,adt_grid_search.best_params_,adt_grid_search.best_score_)\n",
    "print sep2\n",
    "# pick a winner\n",
    "best_adtclf = adt_grid_search.best_estimator_\n",
    "\n",
    "'''\n",
    "\n",
    "## Evaluate Tuned Classifiers\n",
    "\n",
    "print 'Evaluate Tuned Classifiers\\n'\n",
    "\n",
    "#adt_pipe = Pipeline(steps=[('pca',pca),('adaboost_dt',best_adtclf)])\n",
    "\n",
    "dt_pipe = Pipeline(steps=[('pca',pca),('dt',best_dtclf)])\n",
    "\n",
    "best_a_pipe = Pipeline(steps=[('pca',pca),('adaboost',best_aclf)])\n",
    "\n",
    "# including GaussianNB because it is \"tuned\" through feature selection\n",
    "# it will be interesting to see the result of features selected by k_best piped through PCA\n",
    "\n",
    "print 'GaussianNB\\n'\n",
    "test_classifier(g_pipe,my_dataset,my_features_list, scale_features = True, std_features= False)\n",
    "print sep2\n",
    "\n",
    "print 'best_dt_clf\\n'\n",
    "test_classifier(dt_pipe,my_dataset,my_features_list, scale_features = True, std_features= False)\n",
    "print sep2\n",
    "\n",
    "print 'best_a_clf\\n'\n",
    "test_classifier(best_a_pipe,my_dataset,my_features_list, scale_features = True, std_features= False)\n",
    "#print sep2\n",
    "\n",
    "#print 'best_adt_clf\\n'\n",
    "#test_classifier(adt_pipe,my_dataset,my_features_list, scale_features = True, std_features= False)\n",
    "print sep\n",
    "\n",
    "### Setup best adaboost classifier and pipeline for quick analysis by reviewer in tester.py\n",
    "\n",
    "best_a_pipe_scale = Pipeline(steps=[('scale_features',preprocessing.MinMaxScaler()),('pca',PCA(n_components=.95)),('adaboost',best_aclf)])\n",
    "\n",
    "print 'Best adaboost classifier with scaled pipeline run through tester.py\\n'\n",
    "#import tester.py that does not incorporate scaling, as opposed to tester.scale.py above\n",
    "import tester\n",
    "tester.test_classifier(best_a_pipe_scale,my_dataset,my_features_list)\n",
    "print sep\n",
    "\n",
    "### Dump classifier for use in final algorithm poi_id\n",
    "\n",
    "pickle.dump(best_a_pipe_scale, open('best_clf_pipe.pkl', \"w\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Selection by k_best_features\n",
      "\n",
      "10 best features in descending order: ['exercised_stock_options' 'total_stock_value' 'bonus' 'salary'\n",
      " 'deferred_income' 'poi_ratio' 'long_term_incentive' 'restricted_stock'\n",
      " 'total_payments' 'shared_receipt_with_poi']\n",
      "\n",
      "                   feature      score  percent_nan\n",
      "0  exercised_stock_options  24.815080         29.9\n",
      "1        total_stock_value  24.182899         13.2\n",
      "2                    bonus  20.792252         43.8\n",
      "3                   salary  18.289684         34.7\n",
      "4          deferred_income  11.458477         66.7\n",
      "5                poi_ratio  10.019415         40.3\n",
      "6      long_term_incentive   9.922186         54.9\n",
      "7         restricted_stock   9.212811         24.3\n",
      "8           total_payments   8.772778         14.6\n",
      "9  shared_receipt_with_poi   8.589421         40.3\n",
      "\n",
      "##############################################################################################\n",
      "best_a_clf\n",
      "\n",
      "Pipeline(steps=[('scale_features', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, n_components=0.95, whiten=False)), ('adaboost', Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=0.95, whiten=False)), ('adaboost', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=2,\n",
      "          n_estimators=5, random_state=None))]))])\n",
      "\tAccuracy: 0.89220\tPrecision: 0.67914\tRecall: 0.36300\tF1: 0.47312\tF2: 0.40026\n",
      "\tTotal predictions: 15000\tTrue positives:  726\tFalse positives:  343\tFalse negatives: 1274\tTrue negatives: 12657\n",
      "\n",
      "##############################################################################################\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import test_classifier, dump_classifier_and_data\n",
    "\n",
    "#additional imports\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import enron_tools\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "sep = '##############################################################################################'\n",
    "sep2 = '++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++'\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### create list of functions for use as argument to add_features function\n",
    "add_feature_function_list = [enron_tools.add_poi_to_ratio,enron_tools.add_poi_from_ratio,enron_tools.add_poi_interaction_ratio]\n",
    "\n",
    "## add features to data_dict\n",
    "enron_tools.add_features(add_feature_function_list,data_dict)\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "data_label = 'poi'\n",
    "features_list = enron_tools.get_features(data_dict)\n",
    "\n",
    "\n",
    "## email address does not help with prediction and causes exeception, remove\n",
    "features_list.remove('email_address')\n",
    "\n",
    "\n",
    "##remove the data_label so that it can be re-added as the first feature element\n",
    "features_list.remove('poi')\n",
    "\n",
    "##reassemble feaures with the data label as the first element\n",
    "features_list = [data_label] + features_list\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "\n",
    "outliers = ['TOTAL','THE TRAVEL AGENCY IN THE PARK']\n",
    "\n",
    "enron_tools.remove_outliers(data_dict, outliers)\n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "### Continue Feature Selection and dimensionality reduction via get_k_best\n",
    "\n",
    "## get k (k represents number of features) best features\n",
    "k = 10\n",
    "k_best_features = enron_tools.get_k_best(data_dict,features_list,k)\n",
    "\n",
    "print sep\n",
    "\n",
    "# assemble feature list\n",
    "my_features_list = [data_label] + list(k_best_features.feature.values)\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "### extract features and labels for gridsearch optimization\n",
    "\n",
    "# data extraction using k_best features list\n",
    "data = featureFormat(my_dataset, my_features_list, sort_keys = True)\n",
    "\n",
    "tru, trn = targetFeatureSplit(data)\n",
    "\n",
    "## scale extracted features\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "trn = scaler.fit_transform(trn)\n",
    "\n",
    "\n",
    "# Set up cross validator (will be used for tuning all classifiers)\n",
    "cv = cross_validation.StratifiedShuffleSplit(tru,\n",
    "                                            n_iter = 10,\n",
    "                                             random_state = 42)\n",
    "\n",
    "## Evaluate Final Adaboost Classifier\n",
    "\n",
    "# load tuned classifier pipeline\n",
    "\n",
    "\n",
    "best_a_pipe = pickle.load(open('best_clf_pipe.pkl', \"r\") )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print 'best_a_clf\\n'\n",
    "best_a_pipe\n",
    "test_classifier(best_a_pipe,my_dataset,my_features_list)\n",
    "print sep\n",
    "\n",
    "### Dump your classifier, dataset, and features_list so \n",
    "### anyone can run/check your results.\n",
    "\n",
    "dump_classifier_and_data(best_a_pipe, my_dataset, my_features_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
